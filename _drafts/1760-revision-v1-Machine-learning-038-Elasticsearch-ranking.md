---
id: 1761
title: 'Machine learning &#038; Elasticsearch ranking'
date: 2015-11-25T22:02:26+00:00
author: Jilles
layout: revision
guid: http://www.jillesvangurp.com/2015/11/25/1760-revision-v1/
permalink: /2015/11/25/1760-revision-v1/
---
Recently, I went to the monthly search meetup in Berlin and one of the presenters was presenting his plan to use machine learning to improve ranking. He pretty much was asking the meetup group for feedback on his plan and we started discussing.

My opinion on this topic has evolved somewhat over the past years. But fundamentally, it still boils down to this: machine learning is not pixie dust and applying it successfully is actually a really hard problem that is mostly non technical in the sense that it requires a quite rigorous approach towards testing. However, unlike a few years ago, machine learning is becoming more mainstream and the associated technology is increasingly straightforward to integrate for an informed, non expert like myself. A few years ago using machine learning meant you ideally recruited some Ph. D. type engineers with relevant research experience to essentially help you build tooling, algorithms, etc. This is no longer the case and the range of problems you can solve fairly easily with off the shelf technology is increasing.

I've seen machine learning misapplied and the result is usually hard to engineer and maintain software system that doesn't do an obviously better job (or in some cased worse) than manually tuned systems. When such systems misbehave, you are basically in for a lot of iterative trial and error style tuning of your model, learning data set, and feature extraction because there are no easy ways to change the way the model produces outcomes. This is not fundamentally different than how we evolve manually tuned systems. The whole point of machine learning should be to avoid such painful processes. If it is misapplied, the symptom is that things are painful. Common mistakes are optimizing your model for the test data rather than the real world or, worse, fixing the test data until the tests pass. In both cases you end up with a model that is less than ideal in the real world. 

In the context of Elasticsearch, applying machine learning is actually a hard problem for two reasons: 1) elasticsearch ranks based on floating point scores calculated for each search result; 2) the ranking calculation is distributed to the shards and performed without full knowledge of what is happening on other shards. So any machine learning algorithm in the context of this operational model would have to essentially learn how to score documents correctly against a query and then rely on Elasticsearch's result aggregation (sort by score basically) to get good results. This in a nutshell was what the person at the meetup was suggestion as an approach.

This approach is problematic and does not play to the strengths of what machine learning is typically good at, which is classifying or tagging things. Or what Elasticsearch is good at: ranking things efficiently. Asking a machine learning algorithm to give you a floating point score for e.g. 1000 results and then hoping the best one comes up on top is going to end in tears for a single reason: results that have scores close together are effectively depending on a machine learning algorithm to correctly come up with scores with a very low margin of error. If you have 1000 results, it requires less than 0.001 precision for the algorith to correctly rank stuff (assuming evenly distributed scores between 0 and 1). This is a much harder problem than having a machine learning algorithm that can tell you which of two results best matches a query. In reality things are actually worse since you will have clusters of similarly ranked results where the smallest difference in scores will completely change the ranking.

The flaw with the reasoning so far is assuming that ranking all the results is the problem. It is not. When ranking search results, what matters most is the top ranked items. Usually, nobody cares particularly about the order of element 51 and 52. This means that the value of accuracy drops off rapidly as you get more results. It's more important to focus on the top five than to obsess about what happens on page 20 of your search results.

This points us to a solution: don't use machine learning for ranking all results and instead use a hybrid approach of preselecting top results using a hand crafted query. Then you can machine rank the top n results in memory using a simple algorithm of pairwise comparing and sorting the elements and using traditional hand crafted ranking for any further results. A hybrid approach like this would essentially mean reordering the first page of your results, which is where you would hope your best results are anyway. The order on that page is what you are optimizing. The order of the rest can be much more sloppy.

