---
id: 648
title: 'Maven: Follow up'
date: 2009-10-22T13:33:29+00:00
author: Jilles
layout: revision
guid: http://www.jillesvangurp.com/2009/10/22/646-revision-2/
permalink: /2009/10/22/646-revision-2/
---
In my previous post I spent a lot of words criticizing maven. You'd be right to criticize me for blaming maven. However, that would be the wrong way to take my criticism. In my view, maven is a symptom of a bigger underlying problem: the java server side world is bloated with tools, frameworks, application servers, and other stuff designed to address tiny problems with each other. Together, they sort of work but it isn't pretty. What if we'd wipe all of that away, very much like the Sun people did when they designed Java 20 years ago? What would be different? What would be the same? I cannot of course see this topic separately from my previous career as a software engineering researcher. In my view there have been a lot of ongoing developments in the past 20 years that are now converging and morphing into something that could radically improve over the existing state of the art.

Architecture

Lets start with the architecture level. Java packages were a mistake, which is now widely acknowledged. .Net namespaces are arguably better and OSGi bundles with explicit required and provided APIs as well as API versioning are better still. Essentially what I am talking about is dynamic software architecture. The subject has been studied extensively and I see OSGi as the most successful implementation to date that preserves important features that most development environments currently lack or half improvise. The main issue with OSGi is that it layers stuff on top of Java but is not really a part of it. Hence you end up with a mix of manifest files that go into jar files; annotations that go into your source code; and cruft in the form of framework extensions to hook everything up, complete with duplicate functionality for logging, publish subscribe patterns, and even web service frameworks. The OSGi people are moving away towards a more declarative approach. Bring this to its ultimate conclusion and you end up with language level support for basically all that OSGi is trying to do. So, explicit provided and required APIs, API versioning, events, dynamic loading/unloading, isolation. 

A nice feature of Java that OSGi relies on is the classloader. When used properly, it allows you to create a classloader, let it load classes, execute the funtionality, and then destroy the classloader and all the stuff it loaded which is then garbage collected. This is nice for both dynamic loading and unloading of functionality as well as isolating functionality (for security and stability reasons). OSGi heavily depends on this feature and many application servers try to use this. However, the mechanisms used are not exactly bullet proof and there exist enormous problems with e.g. memory leaking which causes engineers to be very conservative with relying on these mechanisms in a live environment. Clearly, we want run-time dynamism in Java++. We want it as part of our architecture but it needs to be more robust and dependable. How hard can that be?

Language

Twenty years ago, Java was a pretty minimalistic language that took basically the best of 20 years (before that) of OO languages and kept a useful subset. Inevitably, lots got discarded or not considered at all. Some mistakes were made, and the language over time absorbed some less than perfect versions of the stuff that didn't make it. So, Java has no language support for properties, this is sort of added on by setter/getter convention introduced for JavaBeans. It has inner classes instead of closures. It has no pure generics but some complicated syntactic sugar that gets compiled to non generic code. The initial concurrent programming concepts in the language were complex, broken, and dangerous to use. Subsequent versions tweaked the semantics and added some useful things like the java concurrent package. The language is overly verbose and 20 years after the fact there is now quite a bit of competition from languages that basically don't suffer from all this. The good news is that most of those have implementations on top of the JVM. Lets not let this degenerate into a language war but clearly the language needs a proper upgrade. IMHO scala could be a good direction but it too has already some compromise embedded and lacks support for the architectural features discussed above. Message passing and functional programming concepts are now seen as important features for scalability. These are tedious at best in Java and Scala supports these well while simultaneously providing a much more concise syntax. Lets just say a replacement of the Java language is overdue. 

Artifacts are transient

So we now have a hypothetical language, with support for all of the above, lets not linger on the details and move on to deployment and run time. Basically the word compile comes from the early days of computing when people had to punch holes into cards and than compile those into stacks and hand feed them to noisy machines. In other words, compilation is a tedious necessary evil. Java popularized the notion of just in time compilation and partial, dynamic compilation. IDEs tend to compile as you edit.  The main difference here is that just in time compilation merely moves the compilation step to the moment the class is loaded whereas dynamic compilation goes a few steps further and takes into account run-time context to decide if and how to compile. In other words, it's something that should just happen and not something that needs to be part of the deployment process. There's no real technical reason to compile ahead of time beyond the minor one time effort that might affect startup. So, for most applications, the notion of generating binary artifacts before they are needed is redundant. This is true for both statically compiled or interpreted. A modern Java system basically uses some binary intermediate format that is generated before runtime. That too is redundant. If you have dynamic compilation, you can just take the source code and execute it (while generating any needed artifacts for that on the fly). You can still do in IDE compilation for validation and static analysis purposes. The distinction between interpreted and static compiled languages has become outdated and as scripting languages show, not having to juggle binary artifacts simplifies life quite a bit. In other words, artifacts are transient and with the transformation from code to running code automated and happening at run time, they should no longer be a consideration.

That means no more build tools.

Without the need to transform artifacts ahead of run-time, the need for tools doing and orchestrating this also changes. Much of what maven does is basically generating, copying, packaging, gathering, etc. artifacts. It's actually pretty stupid work. With most of those artifacts redundant, why keep maven around at all? The answer to that is of course testing and continuous integration as well as application life cycle management and other good practices (like generating documentation). Except, lots of other different tools are involved with that as well. Your IDE is where you'd ideally review problems and issues. Something like Hudson playing together with your version management tooling is where you'd expect continuous integration to take place and application life cycle management is something that is part of your deployment environment. Architectural features of the language and run-time combined with good built in application and component life cycle removes much of the need of external tooling to support all this and improves interoperability.

Source files need to go as well.

Visual age and smalltalk pioneered the notion of non file based program storage where you modify the artifacts in some kind of DB. Intentional programming research basically is about the notion that programs are essentially just interpretations of more abstract things that get transformed (just in time) to executable code or into different views (editable in some cases). That's something to explore a bit further here. Basically, deployment is just a transformation. And checking out, editing and committing is just about having some collaborative editing environment. File based storage is so nineteen seventies. Your code should never leave the cloud (although you might want to keep a local replica for performance/connectivity reasons). We're so close to radically changing the model here. Remove files from the equation and what is left there for maven to do? Surely tools like mave would never survive such a transition.

Anyway, just some reflections on where we are and where we need to go. Java did a lot of pioneering work in a lot of different domains but it is time to move on. Most people selling silver bullets in the form of maven, ruby, continuous integration, are stuck in the current thinking. 